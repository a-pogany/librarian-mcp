#!/usr/bin/env python3
"""
Comprehensive RAG System Test
Tests all enterprise RAG features:
- Hierarchical chunking
- E5-large-v2 embeddings
- Two-stage reranking
- Query caching
- BM25 keyword search
- RRF hybrid fusion
"""

import sys
import time
from pathlib import Path
from typing import Dict, List, Any

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent))

from core.embeddings import EmbeddingGenerator
from core.vector_db import VectorDatabase
from core.chunking import DocumentChunker
from core.reranker import Reranker
from core.cache import QueryCache
from core.bm25_search import BM25Search
from core.hybrid_search import HybridSearchEngine
from core.semantic_search import SemanticSearchEngine
from core.search import SearchEngine
from core.indexer import FileIndexer, DocumentIndex


class TestResults:
    def __init__(self):
        self.tests = []
        self.passed = 0
        self.failed = 0
        self.start_time = time.time()

    def add_test(self, name: str, passed: bool, message: str = "", duration: float = 0):
        self.tests.append({
            'name': name,
            'passed': passed,
            'message': message,
            'duration': duration
        })
        if passed:
            self.passed += 1
        else:
            self.failed += 1

    def print_summary(self):
        total_time = time.time() - self.start_time
        print("\n" + "="*80)
        print("RAG SYSTEM TEST SUMMARY")
        print("="*80)
        print(f"Total Tests: {self.passed + self.failed}")
        print(f"‚úÖ Passed: {self.passed}")
        print(f"‚ùå Failed: {self.failed}")
        print(f"‚è±Ô∏è  Total Time: {total_time:.2f}s")
        print("="*80)

        if self.failed > 0:
            print("\nFAILED TESTS:")
            for test in self.tests:
                if not test['passed']:
                    print(f"  ‚ùå {test['name']}: {test['message']}")

        print("\nDETAILED RESULTS:")
        for test in self.tests:
            status = "‚úÖ PASS" if test['passed'] else "‚ùå FAIL"
            duration_str = f"({test['duration']:.3f}s)" if test['duration'] > 0 else ""
            print(f"  {status} {test['name']} {duration_str}")
            if test['message']:
                print(f"       {test['message']}")


def test_e5_embeddings(results: TestResults):
    """Test E5-large-v2 embeddings with query/passage prefixes"""
    print("\nüìä Testing E5-large-v2 Embeddings...")
    start = time.time()

    try:
        generator = EmbeddingGenerator(model_name="intfloat/e5-large-v2")

        # Test dimension
        query_emb = generator.encode_query("test query")
        assert query_emb.shape == (1024,), f"Expected 1024d, got {query_emb.shape}"

        # Test batch encoding
        docs = ["Document 1", "Document 2", "Document 3"]
        doc_embs = generator.encode(docs)
        assert doc_embs.shape == (3, 1024), f"Expected (3, 1024), got {doc_embs.shape}"

        # Test query prefix
        assert generator.use_query_prefix == True, "E5 model should use query prefix"

        duration = time.time() - start
        results.add_test(
            "E5-large-v2 Embeddings",
            True,
            f"1024-dimensional embeddings, query/passage prefixes enabled",
            duration
        )
        print(f"  ‚úÖ E5 embeddings working (1024d) - {duration:.3f}s")

    except Exception as e:
        results.add_test("E5-large-v2 Embeddings", False, str(e))
        print(f"  ‚ùå E5 embeddings failed: {e}")


def test_hierarchical_chunking(results: TestResults):
    """Test hierarchical document chunking"""
    print("\nüìÑ Testing Hierarchical Chunking...")
    start = time.time()

    try:
        chunker = DocumentChunker(chunk_size=512, chunk_overlap=128)

        # Create test DOCX (simulate with text content)
        test_content = "# Section 1\n\n" + "Content paragraph. " * 200 + "\n\n## Subsection\n\n" + "More content. " * 100

        # Test chunk creation
        chunks = chunker._create_overlapping_chunks(test_content, "Section 1", 1, 0)

        assert len(chunks) > 1, "Should create multiple chunks for long content"

        # Verify overlap
        if len(chunks) > 1:
            chunk1_end = chunks[0]['content'][-50:]
            chunk2_start = chunks[1]['content'][:50]
            # Some overlap expected

        duration = time.time() - start
        results.add_test(
            "Hierarchical Chunking",
            True,
            f"Created {len(chunks)} chunks with 128-token overlap",
            duration
        )
        print(f"  ‚úÖ Chunking working ({len(chunks)} chunks created) - {duration:.3f}s")

    except Exception as e:
        results.add_test("Hierarchical Chunking", False, str(e))
        print(f"  ‚ùå Chunking failed: {e}")


def test_persistent_vector_db(results: TestResults):
    """Test persistent ChromaDB storage"""
    print("\nüíæ Testing Persistent Vector Database...")
    start = time.time()

    try:
        import tempfile
        import shutil

        # Create temporary persist directory
        temp_dir = Path(tempfile.mkdtemp())

        try:
            # Create DB with persistence
            db = VectorDatabase(
                persist_directory=str(temp_dir),
                collection_name="test_persist",
                enable_compression=True
            )

            # Add some documents
            generator = EmbeddingGenerator(model_name="intfloat/e5-large-v2")
            docs = ["Test doc 1", "Test doc 2", "Test doc 3"]
            embeddings = generator.encode(docs)

            ids = ["doc1", "doc2", "doc3"]
            metadatas = [{"idx": i} for i in range(3)]

            db.add_documents(ids, embeddings, metadatas)

            # Verify count
            assert db.get_count() == 3, f"Expected 3 docs, got {db.get_count()}"

            # Verify persistence directory exists
            assert temp_dir.exists(), "Persist directory should exist"

            duration = time.time() - start
            results.add_test(
                "Persistent Vector Storage",
                True,
                f"ChromaDB with persistent storage at {temp_dir}",
                duration
            )
            print(f"  ‚úÖ Persistent storage working - {duration:.3f}s")

        finally:
            # Cleanup
            if temp_dir.exists():
                shutil.rmtree(temp_dir)

    except Exception as e:
        results.add_test("Persistent Vector Storage", False, str(e))
        print(f"  ‚ùå Persistent storage failed: {e}")


def test_two_stage_reranking(results: TestResults):
    """Test two-stage reranking with cross-encoder"""
    print("\nüéØ Testing Two-Stage Reranking...")
    start = time.time()

    try:
        reranker = Reranker(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")

        # Test reranking
        query = "machine learning tutorial"
        mock_results = [
            {'id': 'doc1', 'snippet': 'Introduction to machine learning concepts', 'relevance_score': 0.7},
            {'id': 'doc2', 'snippet': 'Python programming basics', 'relevance_score': 0.6},
            {'id': 'doc3', 'snippet': 'Advanced machine learning algorithms', 'relevance_score': 0.65},
        ]

        reranked = reranker.rerank(query, mock_results, top_k=3)

        # Verify reranking scores exist
        assert all('rerank_score' in r for r in reranked), "All results should have rerank_score"

        # Verify order might change (ML docs should rank higher)
        assert reranked[0]['id'] in ['doc1', 'doc3'], "ML-related docs should rank highest"

        duration = time.time() - start
        results.add_test(
            "Two-Stage Reranking",
            True,
            f"Cross-encoder reranking with ms-marco-MiniLM-L-6-v2",
            duration
        )
        print(f"  ‚úÖ Reranking working (cross-encoder applied) - {duration:.3f}s")

    except Exception as e:
        results.add_test("Two-Stage Reranking", False, str(e))
        print(f"  ‚ùå Reranking failed: {e}")


def test_query_cache(results: TestResults):
    """Test query embedding cache"""
    print("\n‚ö° Testing Query Embedding Cache...")
    start = time.time()

    try:
        cache = QueryCache(max_size=100, ttl=300)

        # Test cache miss
        query = "test query"
        key = cache.cache_key(query, param1="value1")
        assert cache.get_embedding(query, param1="value1") is None, "Initial cache should be empty"

        # Test cache set
        import numpy as np
        test_embedding = np.random.rand(1024)
        cache.set_embedding(query, test_embedding, param1="value1")

        # Test cache hit
        cached = cache.get_embedding(query, param1="value1")
        assert cached is not None, "Should retrieve cached embedding"
        assert np.array_equal(cached, test_embedding), "Cached embedding should match"

        # Test stats
        stats = cache.get_stats()
        assert stats['size'] == 1, "Cache should have 1 entry"

        duration = time.time() - start
        results.add_test(
            "Query Embedding Cache",
            True,
            f"LRU cache with 100 entry capacity, {duration*1000:.1f}ms latency",
            duration
        )
        print(f"  ‚úÖ Query cache working (hit/miss tracking) - {duration:.3f}s")

    except Exception as e:
        results.add_test("Query Embedding Cache", False, str(e))
        print(f"  ‚ùå Query cache failed: {e}")


def test_bm25_search(results: TestResults):
    """Test BM25 keyword search algorithm"""
    print("\nüîç Testing BM25 Keyword Search...")
    start = time.time()

    try:
        # Create test documents
        documents = [
            {'id': 'doc1', 'content': 'machine learning and artificial intelligence'},
            {'id': 'doc2', 'content': 'deep learning neural networks'},
            {'id': 'doc3', 'content': 'python programming tutorial'},
        ]

        bm25 = BM25Search(documents)

        # Test search
        results_list = bm25.search("machine learning", top_k=2)

        assert len(results_list) > 0, "Should return results"
        assert results_list[0]['id'] == 'doc1', "Best match should be doc1"
        assert 'bm25_score' in results_list[0], "Should have BM25 score"

        duration = time.time() - start
        results.add_test(
            "BM25 Keyword Search",
            True,
            f"BM25Okapi algorithm, {len(results_list)} results returned",
            duration
        )
        print(f"  ‚úÖ BM25 search working (probabilistic scoring) - {duration:.3f}s")

    except Exception as e:
        results.add_test("BM25 Keyword Search", False, str(e))
        print(f"  ‚ùå BM25 search failed: {e}")


def test_rrf_fusion(results: TestResults):
    """Test Reciprocal Rank Fusion for hybrid search"""
    print("\nüîÑ Testing RRF Hybrid Fusion...")
    start = time.time()

    try:
        from collections import defaultdict

        # Simulate keyword and semantic results
        keyword_results = [
            {'id': 'doc1', 'relevance_score': 0.9},
            {'id': 'doc2', 'relevance_score': 0.7},
            {'id': 'doc3', 'relevance_score': 0.5},
        ]

        semantic_results = [
            {'id': 'doc3', 'relevance_score': 0.95},  # Different ranking
            {'id': 'doc1', 'relevance_score': 0.8},
            {'id': 'doc4', 'relevance_score': 0.6},
        ]

        # Manual RRF calculation
        k = 60
        scores = defaultdict(float)

        for rank, result in enumerate(keyword_results):
            scores[result['id']] += 1.0 / (k + rank)

        for rank, result in enumerate(semantic_results):
            scores[result['id']] += 1.0 / (k + rank)

        # Verify RRF scores
        assert len(scores) == 4, "Should have 4 unique documents"
        assert scores['doc1'] > 0, "doc1 should have RRF score"
        assert scores['doc3'] > 0, "doc3 should have RRF score"

        # Doc1 appears highly in both, should have highest RRF
        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        duration = time.time() - start
        results.add_test(
            "RRF Hybrid Fusion",
            True,
            f"Reciprocal Rank Fusion with k=60, combined {len(scores)} unique docs",
            duration
        )
        print(f"  ‚úÖ RRF fusion working (rank combination) - {duration:.3f}s")

    except Exception as e:
        results.add_test("RRF Hybrid Fusion", False, str(e))
        print(f"  ‚ùå RRF fusion failed: {e}")


def test_end_to_end_rag_search(results: TestResults):
    """Test complete RAG search pipeline with real indexing"""
    print("\nüöÄ Testing End-to-End RAG Search Pipeline...")
    start = time.time()

    try:
        import tempfile
        import shutil

        # Create temporary docs directory
        temp_dir = Path(tempfile.mkdtemp())

        try:
            docs_dir = temp_dir / "docs" / "ml" / "tutorials"
            docs_dir.mkdir(parents=True)

            # Create test documents
            (docs_dir / "intro.md").write_text("""
# Machine Learning Introduction

Machine learning is a subset of artificial intelligence that focuses on training algorithms to learn from data.

## Key Concepts
- Supervised learning
- Unsupervised learning
- Deep learning
""")

            (docs_dir / "python.md").write_text("""
# Python for ML

Python is the most popular language for machine learning development.

## Libraries
- NumPy for numerical computing
- Pandas for data manipulation
- Scikit-learn for ML algorithms
""")

            # Initialize indexer with RAG
            indexer = FileIndexer(
                docs_root=str(temp_dir / "docs"),
                enable_embeddings=True,
                embedding_model="intfloat/e5-large-v2",
                use_reranking=True
            )

            # Build index
            print("    Building index...")
            indexer.build_index()

            # Create semantic search engine
            semantic_engine = SemanticSearchEngine(
                indexer.embedding_generator,
                indexer.vector_db,
                indexer,
                use_reranking=True
            )

            # Test semantic search
            print("    Testing semantic search...")
            semantic_results = semantic_engine.search(
                query="introduction to machine learning",
                max_results=5
            )

            assert len(semantic_results) > 0, "Should return semantic results"
            print(f"    Found {len(semantic_results)} semantic results")

            # Verify top result
            top_result = semantic_results[0]
            assert 'similarity_score' in top_result, "Should have similarity score"
            assert 'file_name' in top_result, "Should have file name"
            print(f"    Top result: {top_result['file_name']} (score: {top_result['similarity_score']:.2f})")

            # Test hybrid search with RRF
            print("    Testing hybrid search with RRF...")
            keyword_engine = SearchEngine(indexer)

            hybrid_engine = HybridSearchEngine(
                keyword_engine=keyword_engine,
                semantic_engine=semantic_engine,
                default_mode='hybrid',
                use_rrf=True
            )

            hybrid_results = hybrid_engine.search(
                query="python machine learning",
                max_results=5
            )

            assert len(hybrid_results) > 0, "Should return hybrid results"
            assert hybrid_results[0]['search_mode'] in ['hybrid', 'hybrid_rrf'], "Should use hybrid mode"
            print(f"    Found {len(hybrid_results)} hybrid results (mode: {hybrid_results[0]['search_mode']})")

            duration = time.time() - start
            results.add_test(
                "End-to-End RAG Pipeline",
                True,
                f"Full pipeline: indexing ‚Üí chunking ‚Üí embeddings ‚Üí reranking ‚Üí RRF fusion ({duration:.1f}s)",
                duration
            )
            print(f"  ‚úÖ End-to-end pipeline working - {duration:.3f}s")

        finally:
            # Cleanup
            if temp_dir.exists():
                shutil.rmtree(temp_dir)

    except Exception as e:
        results.add_test("End-to-End RAG Pipeline", False, str(e))
        print(f"  ‚ùå End-to-end pipeline failed: {e}")


def main():
    print("\n" + "="*80)
    print("ENTERPRISE RAG SYSTEM COMPREHENSIVE TEST")
    print("="*80)
    print("Testing all Phase 1 and Phase 2 enhancements:")
    print("  1. E5-large-v2 embeddings (1024d)")
    print("  2. Hierarchical document chunking")
    print("  3. Persistent ChromaDB storage")
    print("  4. Two-stage reranking (cross-encoder)")
    print("  5. Query embedding cache")
    print("  6. BM25 keyword search")
    print("  7. Reciprocal Rank Fusion (RRF)")
    print("  8. End-to-end pipeline integration")
    print("="*80)

    results = TestResults()

    # Run all tests
    test_e5_embeddings(results)
    test_hierarchical_chunking(results)
    test_persistent_vector_db(results)
    test_two_stage_reranking(results)
    test_query_cache(results)
    test_bm25_search(results)
    test_rrf_fusion(results)
    test_end_to_end_rag_search(results)

    # Print summary
    results.print_summary()

    # Exit with appropriate code
    sys.exit(0 if results.failed == 0 else 1)


if __name__ == "__main__":
    main()
